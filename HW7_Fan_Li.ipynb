{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNAMQn2J2o9SPz/02oincbg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leodavidfan/AI_Books/blob/main/HW7_Fan_Li.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Read the attached EM-GMM.pdf document and run the following notebook\n",
        " MathematicsOfML/EM_GMM.ipynb at main Â· vijaygwu/MathematicsOfMLContribute to vijaygwu/MathematicsOfML development by creating an account on GitHub.\n",
        "In 500 words, explain how the EM algorithm works in the context GMM.**"
      ],
      "metadata": {
        "id": "hGZ9agTlfHKD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GMM (Gaussian Mixture Model) consists multiple clusters which are modeled as independently gaussian distributed. Each cluster has its own center which is the mean, its spread which is the covariance, and the weight which is the prior probability, or the percentage of the data size of the specific cluster in proportional to the entire clusters. Gaussian distributions are used here due to its flexibility, combination for complexity and multi-dimensional to represent real-world problems.\n",
        "\n",
        "EM algorithm was introduced to resolve the dilemma, the lack of prerequisite information to segregate the data into clusters, as we do not know cluster parameters to determine cluster assignments, nor cluster assignments to compute cluster parameters. It initiates cluster centroids randomly with assumptions of clusters parameters, by maximizing the likelihood (posterior probabilities) to update the clusters parameters until convergence conditions are met, which are defined as threshold of likelihood changes.\n",
        "\n",
        "There are four steps for EM algorithms:\n",
        "\n",
        "The first step is initialization, to define what are the initial states. Here uses K-mean algorithm to setup the initial guesses where the clusters are. The code uses random integer samples as centroids. Through calculating the minimum distances from data to each centroid, choose next centroid with probability proportional to the distance squared, to create new centroids. After it follows the maximum 100 iterations to update centroids based on the nearest distance, which is used as the means in GMM.\n",
        "\n",
        "The second step is the E-step, to estimate cluster membership probabilities. Start with computing multivariate Gaussian pdf and return mahalanobis distance, then computes the posterior probabilities with weighted Gaussian probabilities and normalized to get the value of gammas. Here uses soft assignment, which means data points are assigned to multiple clusters, thus the computation is done for each customer and each cluster which creates more nuanced results than other methods produce only hard assignments. Also, the mixing weight automatically adjusts the sizes of clusters, preventing trapping into false states that clusters must be similar size.\n",
        "\n",
        "The third step is the M-step, to update parameters. It recalculates GMM parameters of weight, mean and variance, with inputs of gamma probabilities calculated from previous E-Step and dataset to fit the softly assigned points. It is guaranteed not to decrease the overall data likelihood at each iteration.\n",
        "The fourth step is the iteration. To fit GMM using EM algorithm, it pulls the previous calculation results into iterations to find the highest log_likelihood, meanwhile monitoring the convergence by detecting the change of log likelihood from previous and current with a threshold of 1e-5 and maximum iteration 200 in this case.\n",
        "\n",
        "GMM using EM model for clustering works well. With prerequisite information, the number of cluster K is set to 3 here. The plot of log-likelihood against iteration shows the algorithm effectiveness. However, the algorithm also has its limits. Different initializations could lead to different solutions as it typically tends to find local maximum likelihood instead of global. Interpretation of what would be the optimal K of clusters can be ambiguous.\n"
      ],
      "metadata": {
        "id": "GF6GDqK2fwVa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCyMsO3lfA0T"
      },
      "outputs": [],
      "source": []
    }
  ]
}